{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Mount your gdrive\n",
    "'''\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  _drivedefault = '/content/drive'\n",
    "  drive.mount( _drivedefault )\n",
    "  import sys, os\n",
    "  sys.path.append( \"{}/{}/\".format(_drivedefault, \"My Drive\") )\n",
    "  os.chdir( \"{}/{}/\".format(_drivedefault, \"My Drive\") )\n",
    "  from helpers import *\n",
    "except ImportError:\n",
    "  print(\"ERROR :: failed to import a package ...\")\n",
    "\n",
    "workspaceDir = setWorkspace( \"workspaces/VsCode-Test\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get some info that you care about\n",
    "'''\n",
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage: ')\n",
    "    print('Allocated: ', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "    print(torch.cuda.max_memory_cached(device=None)) # Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.\n",
    "    print(torch.cuda.memory_allocated(device=None)) # Returns the current GPU memory usage by tensors in bytes for a given device.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install colab_ssh on google colab\n",
    "!pip install colab_ssh --upgrade\n",
    "ngrokToken='<Set Up and Insert your token>'\n",
    "password='setyourpassword'\n",
    "!rm -rf ./ngrok\n",
    "from colab_ssh import launch_ssh, init_git\n",
    "launch_ssh(ngrokToken,password)\n",
    "\n",
    "## Optional: if you want to clone a github repository\n",
    "#init_git(githubUrl)"
   ]
  }
 ]
}